{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOk91l5DvGu0JNMqkpirlNW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaffiSatamyan/GeoLifeCLEF-2024-LifeCLEF-CVPR-FGVC-Kaggle-Competition-/blob/main/Preprocess_%2B_visualize_spatial_data_%2B_eda_%2B_xgb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L50cIUlq86up",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b4ca778-3d26-4354-ddae-09d3fe1d9191"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading geolifeclef-2024, 1493525327 bytes compressed\n",
            "[==================================================] 1493525327 bytes downloaded\n",
            "Downloaded and uncompressed: geolifeclef-2024\n",
            "Downloading climateclef, 26735501021 bytes compressed\n",
            "[============                                      ] 6446080000 bytes downloaded"
          ]
        }
      ],
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'geolifeclef-2024:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F64733%2F7762802%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240329%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240329T160258Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5295510d15ece2eeb24c18e6495af1a9bd1762a13cb2b02fe9bfa193e56d87076b7aa5c33197ff150f981f05570d5f3dee3567024cd2afd2de24debe61b83e3a2b337de62691881976ac4479fd24e670a0ff09fc8085a5f6a052bfa280a050e060abb7443b18c5f8ec339baa23335a17936e28abe44820bb7e42ea0de279fd829c812a2666730cab5f1d9d3fcdf023ddf99de4b04b0933db6ecf597111261ab06e3aa0ff88b49da094100deee88e49f4d0c605a16132686390bb0804a33d753daa645f0707a97242294cbfc40cd853b1f1901498982b59d4ee00a61430eccc290d50280a13c8e17477b8faff50618cb3c6d87d543635840c0a14723ae28a7619,climateclef:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4533016%2F7752763%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240329%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240329T160259Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5c92cc3945000ac4aca660d900edbdd86b96932b10f5c5aa782509cb412e3eee99655e9aed7eb65b5c681c04228806db0f2dcb75e4bcd70741079969c27ddb2b5412ac6e725244964494034d1c6479c53e11fd84fafba9405c2d1f64de06d6223c97a25d4bbd038f9e21832ea3c68f4246f96e9d0f817e73fa6ab16bdea278ad069c1d05b349f57e2b6849f728d9bafe0fcf60774461f506e25cb84b2c533d12bd165983f4bf5fb12c8fba5f4798b2f450f3d69772ffb4b44ab0409aeeb707c1ec40e6d90909fb4888f651f70779035d97ac5c9f99686c4d91e3a7eebbbab32fe422328b02084dbdcf9f1a2f7ff0caf56d2205a667c5794ca693356b56e32966'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "import os\n",
        "# from netCDF4 import Dataset\n",
        "import pandas as pd\n",
        "import geopandas as gpd #\n",
        "from shapely.geometry import Polygon, LineString, Point\n",
        "import rasterio\n",
        "from rasterio.plot import show\n",
        "# from rasterio.transform import from_origin\n",
        "from rasterstats import zonal_stats\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "import tqdm\n",
        "\n",
        "root_path = '/kaggle/input'"
      ],
      "metadata": {
        "id": "I5s5NmZX9UY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read rasterfile and Visualize with point vector"
      ],
      "metadata": {
        "id": "tq4DNYYP962W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read rasterfile\n",
        "bio_raster_path_list = glob(root_path + '/climateclef/Climate/BioClimatic_Average_1981-2010/*.tif')\n",
        "climatic_raster_path_list = glob(root_path + '/climateclef/Climate/Climatic_Monthly_2000-2019/*.tif')"
      ],
      "metadata": {
        "id": "s2Y0eNEV9gd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot raster file\n",
        "for i,raster_path in enumerate(climatic_raster_path_list[:2]):\n",
        "    with rasterio.open(raster_path) as src:\n",
        "        plt.figure(figsize = [8,8])\n",
        "        plt.title(raster_path.split(\"/\")[-1])\n",
        "        show(src)"
      ],
      "metadata": {
        "id": "dJkdHUsS9_7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot raster file\n",
        "for i,raster_path in enumerate(bio_raster_path_list[:2]):\n",
        "    with rasterio.open(raster_path) as src:\n",
        "        plt.figure(figsize = [8,8])\n",
        "        plt.title(raster_path.split(\"/\")[-1])\n",
        "        show(src)"
      ],
      "metadata": {
        "id": "IOD6IEmI-GAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize every monitoring site in meta data\n",
        "# Example for 1000 samples\n",
        "# Read meta data\n",
        "train_meta = pd.read_csv(root_path + \"/geolifeclef-2024/GLC24_PA_metadata_train.csv\")\n",
        "train_meta.head()\n",
        "\n",
        "sub_train_meta = train_meta.drop_duplicates('surveyId').sample(n=1000, random_state=42)\n",
        "sub_train_meta.index = range(len(sub_train_meta))\n",
        "\n",
        "# make Point vector\n",
        "point_list = []\n",
        "for i in tqdm.tqdm(range(len(sub_train_meta))):\n",
        "    x,y = sub_train_meta.loc[i, ['lon', 'lat']]\n",
        "    poind_i = Point(x,y)\n",
        "    point_list.append(poind_i)\n",
        "\n",
        "sub_train_meta.loc[:,'geometry'] = point_list\n",
        "\n",
        "# Read meta data\n",
        "test_meta = pd.read_csv(root_path + \"/geolifeclef-2024/GLC24_PA_metadata_test.csv\")\n",
        "test_meta.head()\n",
        "\n",
        "sub_test_meta = test_meta.drop_duplicates('surveyId').sample(n=1000, random_state=42)\n",
        "sub_test_meta.index = range(len(sub_test_meta))\n",
        "\n",
        "# make Point vector\n",
        "point_list = []\n",
        "for i in tqdm.tqdm(range(len(sub_test_meta))):\n",
        "    x,y = sub_test_meta.loc[i, ['lon', 'lat']]\n",
        "    poind_i = Point(x,y)\n",
        "    point_list.append(poind_i)\n",
        "\n",
        "sub_test_meta.loc[:,'geometry'] = point_list"
      ],
      "metadata": {
        "id": "qg04qBeW-F9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sub_meta_gdf = gpd.GeoDataFrame(sub_train_meta, geometry = 'geometry')\n",
        "train_sub_meta_gdf.crs = {'init':'epsg:4326'}\n",
        "train_sub_meta_gdf.head()\n",
        "\n",
        "test_sub_meta_gdf = gpd.GeoDataFrame(sub_test_meta, geometry = 'geometry')\n",
        "test_sub_meta_gdf.crs = {'init':'epsg:4326'}\n",
        "test_sub_meta_gdf.head()\n",
        "\n",
        "# visualize each monitoring site\n",
        "m = train_sub_meta_gdf.drop_duplicates(['lon', 'lat']).explore(color = 'green')\n",
        "test_sub_meta_gdf.drop_duplicates(['lon', 'lat']).explore(m=m, color = 'red')"
      ],
      "metadata": {
        "id": "TAkvIWPI-F7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize with raster file\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "raster_path = bio_raster_path_list[0]\n",
        "with rasterio.open(raster_path) as src:\n",
        "    show(src, ax=ax)\n",
        "train_sub_meta_gdf.drop_duplicates(['lon', 'lat']).plot(ax=ax,color = 'green')\n",
        "test_sub_meta_gdf.drop_duplicates(['lon', 'lat']).plot(ax=ax,color = 'red')"
      ],
      "metadata": {
        "id": "-b9pRu1U-F4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_meta"
      ],
      "metadata": {
        "id": "4IptgO3S-F2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### merge raster data with meta data"
      ],
      "metadata": {
        "id": "7W03PhJI-XO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# two raster file example\n",
        "train_geo_info_gdf = train_sub_meta_gdf.drop_duplicates(['lon', 'lat'])\n",
        "train_geo_info_gdf.index = range(len(train_geo_info_gdf))\n",
        "\n",
        "test_geo_info_gdf = test_sub_meta_gdf.drop_duplicates(['lon', 'lat'])\n",
        "test_geo_info_gdf.index = range(len(test_geo_info_gdf))\n",
        "\n",
        "for raster_path in tqdm.tqdm(bio_raster_path_list[:2]):\n",
        "    bio_raster = rasterio.open(raster_path)\n",
        "\n",
        "    bio_values = zonal_stats(vectors=train_geo_info_gdf.geometry,raster=bio_raster.read(1), affine=bio_raster.transform,stats=['mean'])\n",
        "    bio_values_list = [value['mean'] for value in bio_values]\n",
        "    feature_name = raster_path.split(\"/\")[-1].split(\".\")[0]\n",
        "    train_geo_info_gdf.loc[:,feature_name] = bio_values_list\n",
        "\n",
        "    bio_values = zonal_stats(vectors=test_geo_info_gdf.geometry,raster=bio_raster.read(1), affine=bio_raster.transform,stats=['mean'])\n",
        "    bio_values_list = [value['mean'] for value in bio_values]\n",
        "    feature_name = raster_path.split(\"/\")[-1].split(\".\")[0]\n",
        "    test_geo_info_gdf.loc[:,feature_name] = bio_values_list"
      ],
      "metadata": {
        "id": "kkshKsRM-Fzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_geo_info_gdf.head()"
      ],
      "metadata": {
        "id": "5dJ7BEvM-Fw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_geo_info_gdf.head()"
      ],
      "metadata": {
        "id": "z2TKV2Yf-FuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset merge\n",
        "Utilize the CSV file provided in this competition\n",
        "\n",
        "\n",
        "* landsat timeseries data has missing value\n",
        "* If Landsat timeseries data is collected after the sampling date, it is considered as data not available for the originally intended prediction timeframe. If not treated as missing values, it may lead to data leakage issues.\n",
        "* We need to consider this timeframe issue when merging datasets."
      ],
      "metadata": {
        "id": "kBT_FJY6-fq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* make train dataset"
      ],
      "metadata": {
        "id": "GYpCnfwx_AKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_meta = pd.read_csv(root_path + \"/geolifeclef-2024/GLC24_PA_metadata_train.csv\")"
      ],
      "metadata": {
        "id": "vNdRRIvi-fi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# read average landsat dataset\n",
        "# Simply merge; this set has no missing values.\n",
        "train_bioclimatic = pd.read_csv(root_path + \"/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/Climate/Average 1981-2010/GLC24-PA-train-bioclimatic.csv\")\n",
        "train_bioclimatic_monthly = pd.read_csv(root_path + \"/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/Climate/Monthly/GLC24-PA-train-bioclimatic_monthly.csv\")\n",
        "train_elevation = pd.read_csv(root_path + \"/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/Elevation/GLC24-PA-train-elevation.csv\")\n",
        "train_human_footprint = pd.read_csv(root_path + \"/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/Human Footprint/GLC24-PA-train-human_footprint.csv\")\n",
        "train_land_cover = pd.read_csv(root_path + \"/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/LandCover/GLC24-PA-train-landcover.csv\")\n",
        "train_soilgrid = pd.read_csv(root_path + \"/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/SoilGrids/GLC24-PA-train-soilgrids.csv\")\n",
        "\n",
        "# first merge bioclimatic averate dataset\n",
        "merge_key = 'surveyId'\n",
        "\n",
        "# merge tables\n",
        "train_merged = pd.merge(train_bioclimatic, train_bioclimatic_monthly, on=merge_key)\n",
        "train_merged = pd.merge(train_merged, train_elevation, on=merge_key)\n",
        "train_merged = pd.merge(train_merged, train_human_footprint, on=merge_key)\n",
        "train_merged = pd.merge(train_merged, train_land_cover, on=merge_key)\n",
        "train_merged = pd.merge(train_merged, train_soilgrid, on=merge_key)\n",
        "\n",
        "train_merged"
      ],
      "metadata": {
        "id": "AJp4A9qr-fgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# landsat time series missing value check\n",
        "landsat_example = pd.read_csv(root_path + f\"/geolifeclef-2024/PA-train-landsat_time_series/GLC24-PA-train-landsat_time_series-blue.csv\")\n",
        "landsat_example.head() ## missing value!"
      ],
      "metadata": {
        "id": "zpLSdeZ--feH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We merge climatic timeseries data within a 10-year time window.(4 q * 10 years = 40)\n",
        "# For example, if the survey year was 2018, we merge monthly data from the years 2008 to 2017.\n",
        "\n",
        "def shift_and_slice(array):\n",
        "    for i in range(len(array)):\n",
        "        row = array[i]\n",
        "        nan_indices = np.isnan(row)\n",
        "\n",
        "        rightmost_nan_index = np.argmax(nan_indices)\n",
        "\n",
        "        if np.any(nan_indices):\n",
        "            shifted_row = np.roll(row, len(row) - rightmost_nan_index)\n",
        "            array[i] = shifted_row\n",
        "\n",
        "    return array[:,-(4 * 10):]\n",
        "\n",
        "for monthly_landsat_data in tqdm.tqdm(['blue', 'green', 'red', 'nir', 'swir1', 'swir2']):\n",
        "    landsat_timeseries = pd.read_csv(root_path + f\"/geolifeclef-2024/PA-train-landsat_time_series/GLC24-PA-train-landsat_time_series-{monthly_landsat_data}.csv\")\n",
        "    origin_arr = landsat_timeseries.iloc[:,1:].values\n",
        "    new_arr = shift_and_slice(origin_arr)\n",
        "\n",
        "    window_dataset =  landsat_timeseries.iloc[:,0:1]\n",
        "    window_dataset.loc[:,[f'{monthly_landsat_data}_t-{i+1}' for i in range(40)]] = new_arr\n",
        "\n",
        "    train_merged = pd.merge(train_merged, window_dataset, on=merge_key)"
      ],
      "metadata": {
        "id": "URD6nSzo-fbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_merged"
      ],
      "metadata": {
        "id": "uO4U0MB8-fZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# species id range from 1 to 11254\n",
        "print(np.max(train_meta.drop_duplicates('speciesId').loc[:,['speciesId']].values), np.min(train_meta.drop_duplicates('speciesId').loc[:,['speciesId']].values))\n",
        "species_target_columns = [str(sp_id) for sp_id in range(1,11255)]\n",
        "input_columns = train_merged.columns.tolist()[1:]\n",
        "\n",
        "# reshape metadata\n",
        "surveyId_list = train_meta.drop_duplicates('surveyId').loc[:,['surveyId']].values\n",
        "target_arr = np.zeros([len(surveyId_list),11254])\n",
        "for i,survey_id in tqdm.tqdm(enumerate(surveyId_list)):\n",
        "    presense_species_ids = train_meta.loc[train_meta.surveyId == survey_id[0],'speciesId'].values\n",
        "    target_arr[i,[int(ids)-1 for ids in presense_species_ids]] = 1\n",
        "\n",
        "train_target_df = pd.DataFrame(target_arr)\n",
        "train_target_df.columns = species_target_columns\n",
        "train_target_df.loc[:,'surveyId'] = surveyId_list\n",
        "train_target_df"
      ],
      "metadata": {
        "id": "1dFVxVPp-fWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* spatial interpolation for missing values"
      ],
      "metadata": {
        "id": "IrzRCtc6_D4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check missing values\n",
        "missing_value_col_list = []\n",
        "rows_with_missing_values = {}\n",
        "for column in train_merged.columns:\n",
        "    missing_rows = train_merged.index[train_merged[column].isnull()].tolist()\n",
        "    if len(missing_rows) > 0:\n",
        "        rows_with_missing_values[column] = len(missing_rows)\n",
        "        missing_value_col_list.append(column)\n",
        "\n",
        "print(rows_with_missing_values)"
      ],
      "metadata": {
        "id": "FK_w8r4m-fUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check missing value region\n",
        "target_colname = 'swir2_t-39'\n",
        "sub_df = pd.merge(train_merged.loc[:, ['surveyId', target_colname]], train_meta.loc[:, ['lon', 'lat', 'surveyId']].drop_duplicates(\"surveyId\"), on='surveyId')\n",
        "na_ind = sub_df.loc[:,target_colname].isna().values\n",
        "\n",
        "sub_df_train = sub_df.loc[~na_ind,:]\n",
        "sub_df_target = sub_df.loc[na_ind,:]\n",
        "x,y,z = sub_df_train['lon'].values, sub_df_train['lat'].values, sub_df_train[target_colname].values\n",
        "x_target,y_target,z_target = sub_df_target['lon'].values, sub_df_target['lat'].values, np.zeros(sub_df_target[target_colname].shape)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(x, y, marker='o', color='green', label='Non Missing Data')\n",
        "plt.scatter(x_target, y_target, marker='o', color='red', label='Missing Data', s = 5)\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NZsYNtXB-fRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check missing value region\n",
        "target_colname = 'Elevation'\n",
        "sub_df = pd.merge(train_merged.loc[:, ['surveyId', target_colname]], train_meta.loc[:, ['lon', 'lat', 'surveyId']].drop_duplicates(\"surveyId\"), on='surveyId')\n",
        "na_ind = sub_df.loc[:,target_colname].isna().values\n",
        "\n",
        "sub_df_train = sub_df.loc[~na_ind,:]\n",
        "sub_df_target = sub_df.loc[na_ind,:]\n",
        "x,y,z = sub_df_train['lon'].values, sub_df_train['lat'].values, sub_df_train[target_colname].values\n",
        "x_target,y_target,z_target = sub_df_target['lon'].values, sub_df_target['lat'].values, np.zeros(sub_df_target[target_colname].shape)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(x, y, marker='o', color='green', label='Non Missing Data')\n",
        "plt.scatter(x_target, y_target, marker='o', color='red', label='Missing Data')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_wpbH9VL-fPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check missing value region\n",
        "target_colname = 'HumanFootprint-Roads'\n",
        "sub_df = pd.merge(train_merged.loc[:, ['surveyId', target_colname]], train_meta.loc[:, ['lon', 'lat', 'surveyId']].drop_duplicates(\"surveyId\"), on='surveyId')\n",
        "na_ind = sub_df.loc[:,target_colname].isna().values\n",
        "\n",
        "sub_df_train = sub_df.loc[~na_ind,:]\n",
        "sub_df_target = sub_df.loc[na_ind,:]\n",
        "x,y,z = sub_df_train['lon'].values, sub_df_train['lat'].values, sub_df_train[target_colname].values\n",
        "x_target,y_target,z_target = sub_df_target['lon'].values, sub_df_target['lat'].values, np.zeros(sub_df_target[target_colname].shape)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(x, y, marker='o', color='green', label='Non Missing Data')\n",
        "plt.scatter(x_target, y_target, marker='o', color='red', label='Missing Data', s = 5)\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KJqb0iol-fM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check missing value region\n",
        "target_colname = 'Soilgrid-bdod'\n",
        "sub_df = pd.merge(train_merged.loc[:, ['surveyId', target_colname]], train_meta.loc[:, ['lon', 'lat', 'surveyId']].drop_duplicates(\"surveyId\"), on='surveyId')\n",
        "na_ind = sub_df.loc[:,target_colname].isna().values\n",
        "\n",
        "sub_df_train = sub_df.loc[~na_ind,:]\n",
        "sub_df_target = sub_df.loc[na_ind,:]\n",
        "x,y,z = sub_df_train['lon'].values, sub_df_train['lat'].values, sub_df_train[target_colname].values\n",
        "x_target,y_target,z_target = sub_df_target['lon'].values, sub_df_target['lat'].values, np.zeros(sub_df_target[target_colname].shape)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(x, y, marker='o', color='green', label='Non Missing Data')\n",
        "plt.scatter(x_target, y_target, marker='o', color='red', label='Missing Data', s = 5)\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a3anlA85-fKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Based on observations from the plot, it appears that there are densely populated measurements near the missing regions.\n",
        "* To handle interpolation, a straightforward approach involves using the values from the nearest neighbors. If a more reliable method is desired, considering techniques such as kriging is also worth exploring."
      ],
      "metadata": {
        "id": "9rWix7LY_Sm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# missing_value_col_list\n",
        "for target_colname in tqdm.tqdm(missing_value_col_list):\n",
        "\n",
        "    sub_df = pd.merge(train_merged.loc[:, ['surveyId', target_colname]], train_meta.loc[:, ['lon', 'lat', 'surveyId']].drop_duplicates(\"surveyId\"), on='surveyId')\n",
        "    na_ind_bool = sub_df.loc[:,target_colname].isna().values\n",
        "\n",
        "    na_ind = sub_df.index[na_ind_bool]\n",
        "    no_na_ind = sub_df.index[~na_ind_bool]\n",
        "\n",
        "\n",
        "\n",
        "    given_points = sub_df.iloc[no_na_ind,[2,3]].values\n",
        "\n",
        "    for na_ind_i in na_ind:\n",
        "\n",
        "        target_point = sub_df.iloc[na_ind_i,[2,3]].values\n",
        "        distances = np.linalg.norm(given_points - target_point, axis=1)\n",
        "        closest_index = no_na_ind[np.argmin(distances)]\n",
        "        closest_value = sub_df.iloc[closest_index,1]\n",
        "        train_merged.loc[na_ind_i, target_colname] = closest_value\n"
      ],
      "metadata": {
        "id": "dHnDNvo1-fHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check missing values\n",
        "rows_with_missing_values = {}\n",
        "for column in train_merged.columns:\n",
        "    missing_rows = train_merged.index[train_merged[column].isnull()].tolist()\n",
        "    if len(missing_rows) > 0:\n",
        "        rows_with_missing_values[column] = len(missing_rows)\n",
        "print(rows_with_missing_values)"
      ],
      "metadata": {
        "id": "rSM93ZvX-fFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check -inf values\n",
        "columns_with_negative_inf = train_merged.columns[train_merged.apply(lambda col: any(col == float('-inf')))]\n",
        "if len(columns_with_negative_inf) > 0:\n",
        "    print(\"This column contains a value of -inf\")\n",
        "    for column_name in columns_with_negative_inf:\n",
        "        print(column_name)\n",
        "else:\n",
        "    print(\"no inf value\")\n"
      ],
      "metadata": {
        "id": "L-f-KYjr-fC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace -inf with zero.\n",
        "train_merged.replace({float('-inf'): 0}, inplace=True)\n",
        "\n",
        "# check -inf values\n",
        "columns_with_negative_inf = train_merged.columns[train_merged.apply(lambda col: any(col == float('-inf')))]\n",
        "if len(columns_with_negative_inf) > 0:\n",
        "    print(\"This column contains a value of -inf\")\n",
        "    for column_name in columns_with_negative_inf:\n",
        "        print(column_name)\n",
        "else:\n",
        "    print(\"no inf value\")"
      ],
      "metadata": {
        "id": "gBg8xL6J-fAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* make test dataset"
      ],
      "metadata": {
        "id": "MHCHsgO2_bJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# read average landsat dataset\n",
        "# Simply merge; this set has no missing values.\n",
        "test_bioclimatic = pd.read_csv(root_path + \"/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/Climate/Average 1981-2010/GLC24-PA-test-bioclimatic.csv\")\n",
        "test_bioclimatic_monthly = pd.read_csv(root_path + \"/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/Climate/Monthly/GLC24-PA-test-bioclimatic_monthly.csv\")\n",
        "test_elevation = pd.read_csv(root_path + \"/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/Elevation/GLC24-PA-test-elevation.csv\")\n",
        "test_human_footprint = pd.read_csv(root_path + \"/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/Human Footprint/GLC24-PA-test-human_footprint.csv\")\n",
        "test_land_cover = pd.read_csv(root_path + \"/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/LandCover/GLC24-PA-test-landcover.csv\")\n",
        "test_soilgrid = pd.read_csv(root_path + \"/geolifeclef-2024/EnvironmentalRasters/EnvironmentalRasters/SoilGrids/GLC24-PA-test-soilgrids.csv\")\n",
        "\n",
        "# first merge bioclimatic averate dataset\n",
        "merge_key = 'surveyId'\n",
        "\n",
        "# merge tables\n",
        "test_merged = pd.merge(test_bioclimatic, test_bioclimatic_monthly, on=merge_key)\n",
        "test_merged = pd.merge(test_merged, test_elevation, on=merge_key)\n",
        "test_merged = pd.merge(test_merged, test_human_footprint, on=merge_key)\n",
        "test_merged = pd.merge(test_merged, test_land_cover, on=merge_key)\n",
        "test_merged = pd.merge(test_merged, test_soilgrid, on=merge_key)\n",
        "\n",
        "test_merged"
      ],
      "metadata": {
        "id": "mCUpzar2-e92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We merge climatic timeseries data within a 10-year time window.(4 q * 10 years = 40)\n",
        "# For example, if the survey year was 2018, we merge monthly data from the years 2008 to 2017.\n",
        "for monthly_landsat_data in tqdm.tqdm(['blue', 'green', 'red', 'nir', 'swir1', 'swir2']):\n",
        "    landsat_timeseries = pd.read_csv(root_path + f\"/geolifeclef-2024/PA-test-landsat_time_series/GLC24-PA-test-landsat_time_series-{monthly_landsat_data}.csv\")\n",
        "    origin_arr = landsat_timeseries.iloc[:,1:].values\n",
        "    new_arr = shift_and_slice(origin_arr)\n",
        "\n",
        "    window_dataset =  landsat_timeseries.iloc[:,0:1]\n",
        "    window_dataset.loc[:,[f'{monthly_landsat_data}_t-{i+1}' for i in range(40)]] = new_arr\n",
        "\n",
        "    test_merged = pd.merge(test_merged, window_dataset, on=merge_key)"
      ],
      "metadata": {
        "id": "VbjyNr6l_eVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_merged"
      ],
      "metadata": {
        "id": "FiNCbMnx_eTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* train and prediction example for speciesId 540"
      ],
      "metadata": {
        "id": "Fl196Ta3_jUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "ed95-EvC_eQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "tartget_speciesId = '540'\n",
        "\n",
        "train_df_sub = pd.merge(train_target_df.loc[:,['surveyId', tartget_speciesId]], train_merged, on='surveyId')\n",
        "train_df_sub.head()\n",
        "\n",
        "X, y = train_df_sub.iloc[:,2:].values, train_df_sub.iloc[:,1].values\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=7)"
      ],
      "metadata": {
        "id": "QL4XaaoI_eOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(y)"
      ],
      "metadata": {
        "id": "a4isT1iH_eLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "model = XGBClassifier(objective='binary:logistic',eval_metric=['error','logloss'], verbose = 1)\n",
        "model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=True,)"
      ],
      "metadata": {
        "id": "nTyn-SnY_eI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_merged"
      ],
      "metadata": {
        "id": "LjDrUFvo_eG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* prediction and mapping species distributions"
      ],
      "metadata": {
        "id": "Mf9tq-Yp_ucH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_meta = pd.read_csv(root_path + \"/geolifeclef-2024/GLC24_PA_metadata_test.csv\")\n",
        "X_test = test_merged.iloc[:,1:].values\n",
        "y_pred_proba = model.predict_proba(X_test)[:,1]\n",
        "test_meta.loc[:,tartget_speciesId + \"_prob\"] = y_pred_proba"
      ],
      "metadata": {
        "id": "6cyecdhp_eD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_meta"
      ],
      "metadata": {
        "id": "HXd6kOwN_eBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make Point vector\n",
        "point_list = []\n",
        "for i in tqdm.tqdm(range(len(test_meta))):\n",
        "    x,y = test_meta.loc[i, ['lon', 'lat']]\n",
        "    poind_i = Point(x,y)\n",
        "    point_list.append(poind_i)\n",
        "\n",
        "test_meta.loc[:,'geometry'] = point_list"
      ],
      "metadata": {
        "id": "ISqhY2fb_d-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_meta_gdf = gpd.GeoDataFrame(test_meta, geometry = 'geometry')\n",
        "test_meta_gdf.crs = {'init':'epsg:4326'}"
      ],
      "metadata": {
        "id": "YWgWNsgf_1j_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_meta_gdf.explore(column = tartget_speciesId + \"_prob\", cmap = 'jet', legend = True)"
      ],
      "metadata": {
        "id": "QNCS3aDl_1gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* feature importance"
      ],
      "metadata": {
        "id": "FzKECG1I_79O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer(X_test)\n",
        "features_col_list = train_merged.columns[1:]\n",
        "\n",
        "shap.summary_plot(shap_values, X_test, feature_names = features_col_list)"
      ],
      "metadata": {
        "id": "i9EyNhu2_1dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* feature importance map Bio-pr_04_2013"
      ],
      "metadata": {
        "id": "EDNBAouH_-WW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_feature = 'Bio-pr_04_2013'\n",
        "feature_shap_values = shap_values[:,features_col_list == target_feature].values"
      ],
      "metadata": {
        "id": "U7FyTxsQ_1an"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_shap_values.shape"
      ],
      "metadata": {
        "id": "U1x75g6xADOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_meta_gdf.loc[:,'feature_contribution'] = feature_shap_values"
      ],
      "metadata": {
        "id": "tLBUlGcjADMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_meta_gdf.explore(column = 'feature_contribution', cmap = 'jet', legend = True)"
      ],
      "metadata": {
        "id": "jAk8XYKD-Fq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* feature importance map elevation"
      ],
      "metadata": {
        "id": "tGtZnPeJAHuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Ftarget_feature = 'Elevation'\n",
        "feature_shap_values = shap_values[:,features_col_list == target_feature].values\n",
        "test_meta_gdf.loc[:,'feature_contribution'] = feature_shap_values\n",
        "test_meta_gdf.explore(column = 'feature_contribution', cmap = 'jet', legend = True)"
      ],
      "metadata": {
        "id": "LyajHp31-Fg_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}